{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四章—朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于两个假设：\n",
    "**1、特征之间相互独立（即朴素的含义）**  \n",
    "**2、每个特征同等重要**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、使用python进行文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1、词表到向量的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]                                              # 1 代表侮辱性文字, 0 代表正常言论\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列表合并，并去重\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set()                        # 创建空集合\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) # 两个集合的并集\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对照词列表生成词向量\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)                                  # 定义一个列表表示词向量，初始化为[0, 0,.....0]\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1                    # 输入词集合中出现词列表中的单词，词向量相应位置设为1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOP, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOP)\n",
    "print(myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(setOfWords2Vec(myVocabList, myVocabList))\n",
    "print(setOfWords2Vec(myVocabList, listOP[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2、朴素贝叶斯分类器训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)                        # 训练样本数目\n",
    "    numWords = len(trainMatrix[0])                         # 特征数\n",
    "    pAbusive = np.sum(trainCategory)/float(numTrainDocs)   # 侮辱性言论的比率\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)                              # 初始化分子\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0                                          # 初始化分母\n",
    "    for i in range(numTrainDocs):\n",
    "        if(trainCategory[i] == 1):\n",
    "            p1Num += trainMatrix[i]                        # 正样本中出现的频数\n",
    "            p1Denom += np.sum(trainMatrix[i])              # 所有词条总频数\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += np.sum(trainMatrix[i])\n",
    "    p1Vect = np.log(p1Num/p1Denom)\n",
    "    p0Vect = np.log(p0Num/p1Denom)\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat = []\n",
    "for postinDoc in listOP:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "p0v, p1v, pAb = trainNB0(trainMat, listClasses)\n",
    "print(p0v, p1v, pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3、朴素贝叶斯分类函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ln(p(w0,w1...wn,c1)) = ln(p(w0,w1...wn|c1)) + ln(p(c1))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = np.sum(vec2Classify * p1Vec) + np.log(pClass1)       # vec2Classify为要分类的向量\n",
    "    p0 = np.sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if(p1 > p0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()                               # 加载数据集及标签\n",
    "    myVocabList = createVocabList(listOPosts)                             # 生成词列表\n",
    "    # 将所有训练数据转换为词向量\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses))   # 学习先验概率\n",
    "    testEntry = ['love', 'my', 'dalmation']                               # 测试用例              \n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))            # 测试用例转换为词向量\n",
    "    print(testEntry,'classified as: ',classifyNB(thisDoc, p0V, p1V, pAb)) # 输出测试结果\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4、朴素贝叶斯词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if(word in vocabList):\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、使用朴素贝叶斯过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1、准备数据：切分文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySent = \"This book is the best book on Python or M.L. I have ever laid eyes upon.\"\n",
    "print(mySent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regEx  = re.compile('\\\\W*')                                           # 正则化的方法！！！\n",
    "listofTokens = regEx.split(mySent)\n",
    "listofTokens = [tok for tok in listofTokens if len(tok) > 0]\n",
    "print(listofTokens)\n",
    "listofTokens = [tok.lower() for tok in listofTokens if len(tok) > 0]  # 转换为小写\n",
    "print(listofTokens)\n",
    "listofTokens = [tok.upper() for tok in listofTokens if len(tok) > 0]  # 转换为大写\n",
    "print(listofTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emailText = open('D:/data/study/AI/ML/MLcode/Ch04/email/ham/6.txt').read()\n",
    "listOfTokens = regEx.split(emailText)\n",
    "listOfTokens = [tok for tok in listOfTokens if len(tok) > 0]\n",
    "print(listOfTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2、文件解析及完整的垃圾邮件测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):                                        # 输入为大的字符串，输出为词列表\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] # 输出长度大于2的字符串，并且转化为小写字母"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def spamTest():\n",
    "    docList = []             # 定义存储词列表的列表\n",
    "    classList = []           # 定义类列表\n",
    "    fullText = []            # 将所有词列表合并为一个词列表\n",
    "    for i in range(1, 26):\n",
    "        # 正样本处理\n",
    "        wordList = textParse(open('D:/data/study/AI/ML/MLcode/Ch04/email/spam/%d.txt'% i).read()) # 将文件转换为词列表\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        # 负样本处理\n",
    "        wordList = textParse(open('D:/data/study/AI/ML/MLcode/Ch04/email/ham/%d.txt'% i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)          # 词列表合并，并去重\n",
    "    trainingSet = list(range(50))                 # 正负样本一共50个\n",
    "    testSet=[]                                    # 创建测试集\n",
    "    # 随机构建训练集:10个测试样本，40个训练样本\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet))) # 生成随机数可替换为: random.randint(0, len(trainingSet))\n",
    "        testSet.append(trainingSet[randIndex])               # 将随机选择的训练集放入测试集\n",
    "        del(trainingSet[randIndex])                          # 删除训练集中对应数据\n",
    "    trainMat=[]\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:                                              # 训练分类器\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))       # 根据词袋模型，将训练集词列表转换为词向量\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))    # 训练分类器\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:                                                  # 对测试集分类\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])           # 生成测试词向量\n",
    "        if(classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]):\n",
    "            errorCount += 1                                                   # 分类错误时错误数加一\n",
    "            print(\"classification error\",docList[docIndex])\n",
    "    print('the error rate is: ',float(errorCount)/len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、使用朴素贝叶斯分类器从个人广告中获取区域倾向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1、RSS源分类器及高频词去除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "print(len(ny['entries']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSS源分类器及高频词去除函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList, fullText):\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token] = fullText.count(token)               # count方法统计频数\n",
    "    sortedFreq = sorted(freqDict.items(), key = operator.itemgetter(1), reverse = True) # 按频数从大到小排序\n",
    "    return sortedFreq[:30]                                    # 返回频数最高的30个词条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此处为sorted函数对字典排序的Demo\n",
    "dictNew = {\"A\":2, \"Q\":4, \"B\":7, \"E\":9, \"C\":1}\n",
    "dictOut1 = sorted(dictNew.items(), key = operator.itemgetter(1), reverse = True)\n",
    "dictOut2 = sorted(dictNew.items(), key = lambda x:x[1], reverse = True)\n",
    "print(dictOut1,'\\n',dictOut2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localWords(feed1, feed0):\n",
    "    import feedparser\n",
    "    import random\n",
    "    docList=[]; classList = []; fullText =[]                  # 定义文本列表、类列表、文本合并后的词条列表\n",
    "    minLen = min(len(feed1['entries']), len(feed0['entries']))# 求两个较小值\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])  # 将字符串分割得到词列表\n",
    "        docList.append(wordList)                              # 将词列表整体放入文本列表\n",
    "        fullText.extend(wordList)                             # 将词列表所有成员放入文本词条列表\n",
    "        classList.append(1)                                   # 将feed1标记为class 1\n",
    "        # 与上面同理\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)                      # 创建词条列表\n",
    "    top30Words = calcMostFreq(vocabList, fullText)             # 得到频数最高的前30个词条\n",
    "    for pairW in top30Words:\n",
    "        if(pairW[0] in vocabList):                            # 理解这里为什么是pairW[0]而不是pairW\n",
    "            vocabList.remove(pairW[0])                        # 词条列表中去除高频词条\n",
    "    trainingSet = list(range(2 * minLen))                     # 创建训练集，转化为列表\n",
    "    testSet=[]                                                # 创建测试集\n",
    "    # 随机选择20个样本作为测试集，余下为训练集\n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat=[]\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:                                          # 训练分类器\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))   # 根据词袋模型，将训练集词列表转换为词向量\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat),np.array(trainClasses)) # 训练分类器\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:                                              # 对测试集分类\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1                                               # 分类错误时错误数加一\n",
    "    print('the error rate is: ',float(errorCount)/len(testSet))\n",
    "    return vocabList, p0V, p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "vocabList, pSF, pNY = localWords(ny, sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最具表征性的词汇显示函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopWords(ny, sf):\n",
    "    import operator\n",
    "    vocabList, p0V, p1V=localWords(ny, sf)\n",
    "    topNY=[]\n",
    "    topSF=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if(p0V[i] > -6.0):\n",
    "            topSF.append((vocabList[i], p0V[i]))\n",
    "        if(p1V[i] > -6.0):\n",
    "            topNY.append((vocabList[i], p1V[i]))\n",
    "    sortedSF = sorted(topSF, key = lambda pair: pair[1], reverse = True)\n",
    "    print(\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n",
    "    for item in sortedSF:\n",
    "        print(item[0])\n",
    "    sortedNY = sorted(topNY, key = lambda pair: pair[1], reverse = True)\n",
    "    print(\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\")\n",
    "    for item in sortedNY:\n",
    "        print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTopWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
